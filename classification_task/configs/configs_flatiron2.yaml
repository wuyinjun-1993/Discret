model:
  mlp:
    latent_size: 300
  transformer:
    latent_size: 100
    tf_latent_size: 100
    fix_pretrained_model: true
    depth: 6
    heads: 8
    attn_dropout: 0.1
    ff_dropout: 0.1
rl:
  dqn:
    mem_sample_size: 16
    replay_memory_capacity: 5000
    epsilon: 0.2
    epsilon_falloff: 0.9
    gamma: 0.999
    target_update: 20
    discretize_feat_value_count: 100
  ppo:
    n_updates_per_iteration: 10
    clip: 0.1
    timesteps_per_batch: 5
    continue_act: false
    gamma: 0.999
